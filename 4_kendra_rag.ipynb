{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kendra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "kendra = boto3.client(\"kendra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kendra_index_id = \"e2b2ac6f-0b68-4bd0-8792-96729e04feb7\"\n",
    "model_id = \"cohere.command-r-plus-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_search_query(question: str):\n",
    "\n",
    "    result = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        additionalModelRequestFields={\"search_queries_only\": True},\n",
    "        additionalModelResponseFieldPaths=[\"/search_queries\"],\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": question}],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return list(\n",
    "        map(\n",
    "            lambda x: x[\"text\"],\n",
    "            result[\"additionalModelResponseFields\"][\"search_queries\"],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetching_relevant_documents(queries: list[str]):\n",
    "\n",
    "    items = []\n",
    "    for query in queries:\n",
    "\n",
    "        response = kendra.retrieve(\n",
    "            IndexId=kendra_index_id,\n",
    "            QueryText=query,\n",
    "            AttributeFilter={\n",
    "                \"EqualsTo\": {\"Key\": \"_language_code\", \"Value\": {\"StringValue\": \"ja\"}}\n",
    "            },\n",
    "        )\n",
    "\n",
    "        keys = [\n",
    "            \"Id\",\n",
    "            \"DocumentId\",\n",
    "            \"DocumentTitle\",\n",
    "            \"Content\",\n",
    "            \"DocumentURI\",\n",
    "        ]\n",
    "\n",
    "        items.extend(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda x: {k: v for k, v in x.items() if k in keys},\n",
    "                    response[\"ResultItems\"],\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_response(question: str, documents: list[str]):\n",
    "\n",
    "    result = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        additionalModelRequestFields={\"documents\": documents},\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": question}],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return result[\"output\"][\"message\"][\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"testdataset.json\")\n",
    "\n",
    "question = df[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = generate_search_query(question[0])\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_documents = fetching_relevant_documents(queries)\n",
    "relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generating_response(question=question[0], documents=relevant_documents)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "citations = []\n",
    "\n",
    "for q in question:\n",
    "    queries = generate_search_query(question=q)\n",
    "    relevant_documents = fetching_relevant_documents(queries)\n",
    "    answer = generating_response(question=q, documents=relevant_documents)\n",
    "\n",
    "    answers.append(answer)\n",
    "    citations.append(relevant_documents)\n",
    "\n",
    "    print(q)\n",
    "    print(answer)\n",
    "\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"kendra_answers\"] = answers\n",
    "df[\"kendra_citations\"] = citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"answer_data_kendra.json\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"answer_data_kendra.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = df[\"question\"]\n",
    "answer = df[\"kendra_answers\"]\n",
    "ground_truth = df[\"ground_truth\"]\n",
    "context = []\n",
    "\n",
    "for citations in df[\"kendra_citations\"]:\n",
    "    tmp = []\n",
    "    for citation in citations:\n",
    "        body = citation[\"Content\"]\n",
    "        tmp.append(body)\n",
    "\n",
    "    context.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "eval_data = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"contexts\": context,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "\n",
    "llm = AzureChatOpenAI(azure_deployment=os.getenv(\"AZURE_DEPLOYMENT_GPT4OMINI\", None))\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=os.getenv(\"AZURE_DEPLOYMENT_EMBEDDINGS\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    ")\n",
    "from ragas.run_config import RunConfig\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "run_config = RunConfig(max_wait=600, max_retries=100)\n",
    "\n",
    "result = evaluate(\n",
    "    eval_data,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings,\n",
    "    run_config=run_config,\n",
    "    raise_exceptions=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = result.to_pandas()\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_json(\"eval_data_kendra.json\", force_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
